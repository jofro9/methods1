---
title: "Question 3"
author: "Joseph Froelicher"
date: "10/15/2020"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, include = TRUE)

setwd("C:/Users/froelijo/dev/methods1/hw6")
procedure <- read.csv("ProcedureCost.csv")
```
# Part A
```{r}
observed <- mean(procedure[procedure$Procedure == 1,]$Cost) - mean(procedure[procedure$Procedure == 2,]$Cost)

n1 <- dim(procedure[procedure$Procedure == 1,])[1]
n2 <- dim(procedure[procedure$Procedure == 2,])[1]

B <- 10000

boot_ratio <- vector("double", B)

set.seed(8675309)

for (i in 1:B) {
  val1 <- sample(procedure$Cost, n1, replace = FALSE)
  val2 <- sample(procedure$Cost, n2, replace = FALSE)
  
  boot_ratio[i] <- mean(val1) - mean(val2)
}

hist(boot_ratio, main = "Bootstrap of Permutation of Costs", breaks = 50)
abline(
  v = mean(procedure[procedure$Procedure == 1,]$Cost) - mean(procedure[procedure$Procedure == 2,]$Cost),
  col = 'blue'
)
```

# Part B
```{r}
p_right <- (sum(boot_ratio >= observed) + 1) / (B + 1)
p_left <- (sum(boot_ratio <= -observed) + 1) / (B + 1)
```
p = `r p_right` x 2 = `r p_right * 2` < 0.05. This suggests there is a significat difference in the cost between the new procedure and the old procedure.
  
# Part C
It would not make sense to combine these two conclusions. The permutation test and the bootstrap ratio test are fundamentally different tests, where one samples with replacement and one samples without replacement. This means that we are not sampling the same way between the two tests, and it would be ethically inapropriate to combine the results from the two tests. Mathematically speaking sampling with replacement results in a very different number of combinations, that the number of permutations that come from sampling without replacement.